{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T19:15:24.322202Z",
     "start_time": "2024-11-22T19:15:24.289398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mint.model.transformer import Transformer, TransformerConfig\n",
    "from mint.common import create_config, to_dict\n",
    "from mint.trainer import Trainer, TrainerConfig\n",
    "import os\n",
    "from mint.translator import Translator\n",
    "from mint.dataset import Dataset\n",
    "from mint.tokenizer import Tokenizer"
   ],
   "id": "fd86fb0263d8d8da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T19:25:42.248127Z",
     "start_time": "2024-11-22T19:25:42.188556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_PATH = \"../datasets/en_sk/\"\n",
    "\n",
    "dataset = Dataset.load_preprocessed(DATASET_PATH)\n",
    "source_tokenizer = Tokenizer.load(\"../datasets/en_sk/source_tokenizer/\")\n",
    "target_tokenizer = Tokenizer.load(\"../datasets/en_sk/target_tokenizer/\")\n",
    "source_tokenizer.pad_token = \"<pad>\"\n",
    "target_tokenizer.pad_token = \"<pad>\""
   ],
   "id": "52eb18fa707eec72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[494, 494, 494, 79, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T19:15:48.867566Z",
     "start_time": "2024-11-22T19:15:48.178259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = create_config(TransformerConfig())\n",
    "config.glob.d_model = 512\n",
    "config.glob.n_heads = 8\n",
    "config.glob.max_seq_len = 128\n",
    "config.glob.d_feedforward = 2048\n",
    "config.glob.p_dropout = 0.1\n",
    "\n",
    "config.encoder_config.n_blocks = 10\n",
    "config.encoder_config.vocab_size = 10000\n",
    "config.decoder_config.n_blocks = 10\n",
    "config.decoder_config.vocab_size = 10000\n",
    "\n",
    "model = Transformer(config)\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ],
   "id": "fe589b745cb5fff9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_blocks': 10, 'vocab_size': 10000, 'transformer_block_config': {'d_model': 512, 'd_feedforward': 2048, 'p_dropout': 0.1, 'attention_config': {'n_heads': 8, 'd_model': 512, 'max_seq_len': 128, 'context_window': None}}, 'embedding_config': {'vocab_size': 10000, 'd_model': 512, 'max_seq_len': 128, 'learnable_positional_embeddings': True}}\n",
      "Number of parameters: 126471680\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T19:16:42.813921Z",
     "start_time": "2024-11-22T19:16:38.704440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer_config = create_config(TrainerConfig())\n",
    "trainer_config.logger_config.experiment_name = \"exp2\"\n",
    "trainer_config.warmup_steps = 3000\n",
    "trainer_config.learning_rate = 1e-4\n",
    "trainer = Trainer(model, dataset, **to_dict(trainer_config), source_tokenizer=source_tokenizer, target_tokenizer=target_tokenizer)\n",
    "trainer.train(10)"
   ],
   "id": "8adf32a235f1cd47",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/10 avg loss: 9.2853:   0%|          | 11/28140 [00:03<2:48:20,  2.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m trainer_config\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-4\u001B[39m\n\u001B[1;32m      5\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(model, dataset, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mto_dict(trainer_config), source_tokenizer\u001B[38;5;241m=\u001B[39msource_tokenizer, target_tokenizer\u001B[38;5;241m=\u001B[39mtarget_tokenizer)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fit/zpja/mint/trainer.py:44\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, n_epochs)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m, n_epochs):\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_epochs):\n\u001B[0;32m---> 44\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../checkpoint/epoch_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate(epoch, n_epochs)\n",
      "File \u001B[0;32m~/fit/zpja/mint/trainer.py:66\u001B[0m, in \u001B[0;36mTrainer.train_epoch\u001B[0;34m(self, epoch, n_epochs)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 66\u001B[0m epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mlog_scalar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mlog_scalar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mparam_groups[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T19:55:50.430416Z",
     "start_time": "2024-11-20T19:55:50.300773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translator = Translator(model, source_tokenizer, target_tokenizer)\n",
    "\n",
    "translator.translate(\"Good evening\", max_length=128)"
   ],
   "id": "4ac8876ad2b381ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dobré večera<|endoftext|>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e2432dd8aef37ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
