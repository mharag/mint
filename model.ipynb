{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T12:56:18.210168Z",
     "start_time": "2024-11-18T12:56:16.978722Z"
    }
   },
   "source": [
    "from datasets import load_from_disk\n",
    "from jinja2.lexer import ignored_tokens\n",
    "\n",
    "dataset = load_from_disk(\"preprocessed_dataset/paired_tokenized_dataset\")\n",
    "\n",
    "for i in dataset[\"train\"]:\n",
    "    print(i)\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source_tokens': [38, 82, 79, 77, 221, 87, 72, 79, 77, 221, 69, 86, 69, 82, 89, 221, 70, 65, 77, 73, 76, 89, 221, 73, 78, 221, 72, 69, 65, 86, 69, 78, 221, 65, 78, 68, 221, 79, 78, 221, 69, 65, 82, 84, 72, 221, 84, 65, 75, 69, 83, 221, 73, 84, 83, 221, 78, 65, 77, 69, 14, 0, 0, 0], 'target_tokens': [90, 221, 75, 84, 79, 82, 128, 103, 72, 79, 221, 77, 128, 95, 221, 75, 65, 130, 123, 68, 128, 122, 221, 82, 79, 68, 221, 78, 65, 221, 78, 69, 66, 69, 83, 73, 65, 67, 72, 221, 73, 221, 78, 65, 221, 90, 69, 77, 73, 221, 83, 86, 79, 74, 69, 221, 77, 69, 78, 79, 12, 0, 0, 0]}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T13:07:11.293086Z",
     "start_time": "2024-11-18T13:07:11.266059Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4098a1922219d1d4",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T13:07:11.958538Z",
     "start_time": "2024-11-18T13:07:11.943174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformer = Transformer(config)\n",
    "print(transformer)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in transformer.parameters()))\n",
    "\n",
    "for i in dataset[\"train\"]:\n",
    "    batch = torch.tensor([i[\"source_tokens\"]]).to(next(transformer.parameters()).device)\n",
    "    target_tokens =torch.tensor([[10,20,10,20,10,20,10,20]]).to(next(transformer.parameters()).device)\n",
    "    print(transformer(batch, target_tokens))\n",
    "    print(transformer(batch, target_tokens))\n",
    "    break"
   ],
   "id": "c40752803b5a38df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(\n",
      "      (token_embedding): Embedding(10000, 32)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (multi_head_attention): MultiHeadAttention(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(\n",
      "      (token_embedding): Embedding(10001, 32)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-1): 2 x TransformerBlock(\n",
      "        (multi_head_attention): MultiHeadAttention(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (vocabulary_projection): Linear(in_features=32, out_features=10000, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 989424\n",
      "tensor([[[4.8517e-05, 8.1266e-05, 1.2911e-04,  ..., 1.4110e-04,\n",
      "          4.9560e-05, 6.7328e-05],\n",
      "         [7.0036e-05, 4.0825e-05, 6.2375e-05,  ..., 1.2392e-04,\n",
      "          1.8550e-04, 5.2845e-05],\n",
      "         [1.0476e-04, 1.0776e-04, 6.1699e-05,  ..., 3.7514e-05,\n",
      "          7.4755e-05, 9.3784e-05],\n",
      "         ...,\n",
      "         [1.0613e-04, 8.1000e-05, 1.0044e-04,  ..., 4.1458e-05,\n",
      "          6.5085e-05, 9.0559e-05],\n",
      "         [4.8117e-05, 2.8323e-05, 1.4660e-04,  ..., 1.5442e-04,\n",
      "          1.9455e-04, 6.3424e-05],\n",
      "         [1.9134e-04, 5.7213e-05, 1.0582e-04,  ..., 4.3109e-05,\n",
      "          8.6463e-05, 7.2335e-05]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[5.6826e-05, 6.9520e-05, 1.1132e-04,  ..., 1.7949e-04,\n",
      "          7.8584e-05, 4.9320e-05],\n",
      "         [4.0025e-05, 4.1826e-05, 9.0783e-05,  ..., 1.9346e-04,\n",
      "          2.6350e-04, 5.9794e-05],\n",
      "         [6.7888e-05, 3.6586e-05, 4.2062e-05,  ..., 6.1501e-05,\n",
      "          7.4001e-05, 1.1102e-04],\n",
      "         ...,\n",
      "         [9.6812e-05, 8.8735e-05, 6.2188e-05,  ..., 5.2275e-05,\n",
      "          8.5012e-05, 7.8533e-05],\n",
      "         [3.9977e-05, 3.7812e-05, 1.5306e-04,  ..., 1.6010e-04,\n",
      "          1.8608e-04, 4.8302e-05],\n",
      "         [2.0347e-04, 1.0673e-04, 8.9051e-05,  ..., 6.4778e-05,\n",
      "          4.9327e-05, 6.3001e-05]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T13:07:17.952232Z",
     "start_time": "2024-11-18T13:07:17.875481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, dataset, learning_rate, batch_size):\n",
    "        self.model = model\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.data_loader = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=batch_size)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(self.device)\n",
    "        \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def train(self, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "            self.train_epoch()\n",
    "            self.save_model(f\"checkpoint/epoch_{epoch}.pt\")\n",
    "                \n",
    "    def train_epoch(self):\n",
    "        epoch_loss = 0\n",
    "        steps = 0\n",
    "        for batch in (progress_bar := tqdm(self.data_loader)):\n",
    "            source_tokens = torch.stack(batch[\"source_tokens\"], dim=1).to(self.device)\n",
    "            target_tokens = torch.stack(batch[\"target_tokens\"], dim=1).to(self.device)\n",
    "            #target_tokens = torch.stack([torch.tensor([10,20,10,20,10,20,10,20]) for i in target_tokens]).to(self.device)\n",
    "            #if random.random() < 0.5:\n",
    "            #    target_tokens = torch.tensor([[10, 10, 10] for i in target_tokens]).to(self.device)\n",
    "            #else:\n",
    "            #    target_tokens = torch.tensor([[20, 10, 20] for i in target_tokens]).to(self.device)\n",
    "            \n",
    "            predictions = self.model(source_tokens, target_tokens)[:, :-1]  # there is no ground truth for the last token\n",
    "            print(predictions)\n",
    "            loss = self.loss(predictions.reshape(-1, config[\"vocab_size\"]), target_tokens.view(-1))\n",
    "            print(loss)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            break\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            if steps % 1000 == 0:\n",
    "                print(f\"target: {target_tokens[0]}\")\n",
    "                print(f\"prediction: {torch.argmax(predictions[0], dim=-1)}\")\n",
    "            \n",
    "            steps += 1\n",
    "            progress_bar.desc = f\"avg loss: {epoch_loss / steps:.4f}\"\n",
    "        \n",
    "                \n",
    " \n",
    "trainer = Trainer(transformer, dataset, 1e-6, 128)\n",
    "trainer.train(1)\n",
    "#trainer.load_model(\"checkpoint/epoch_0.pt\")"
   ],
   "id": "9f59cf5c8b0635fc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7035 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5.3051e-05, 6.8222e-05, 1.2062e-04,  ..., 1.8118e-04,\n",
      "          6.9246e-05, 8.5104e-05],\n",
      "         [3.2716e-05, 5.3148e-05, 4.4545e-05,  ..., 1.2875e-04,\n",
      "          7.7368e-05, 3.4301e-05],\n",
      "         [8.5483e-05, 6.5093e-05, 6.6989e-05,  ..., 1.3055e-04,\n",
      "          1.1358e-04, 2.0884e-05],\n",
      "         ...,\n",
      "         [3.3404e-05, 6.8917e-05, 9.3391e-05,  ..., 3.0210e-04,\n",
      "          1.5110e-04, 6.3270e-05],\n",
      "         [3.2458e-05, 4.5867e-05, 8.7771e-05,  ..., 2.1431e-04,\n",
      "          3.3203e-04, 4.7935e-05],\n",
      "         [4.5846e-05, 6.7635e-05, 1.3697e-04,  ..., 1.5476e-04,\n",
      "          2.7897e-04, 6.2755e-05]],\n",
      "\n",
      "        [[8.4789e-05, 9.3145e-05, 1.4359e-04,  ..., 1.6626e-04,\n",
      "          1.2231e-04, 5.1772e-05],\n",
      "         [2.8598e-05, 4.9335e-05, 3.5168e-05,  ..., 1.7987e-04,\n",
      "          1.4791e-04, 2.4233e-05],\n",
      "         [3.4207e-05, 8.9369e-05, 3.8548e-05,  ..., 2.8438e-04,\n",
      "          2.4675e-04, 1.6461e-05],\n",
      "         ...,\n",
      "         [5.0140e-05, 5.9609e-05, 8.5691e-05,  ..., 1.7527e-04,\n",
      "          2.6839e-04, 7.4790e-05],\n",
      "         [3.6266e-05, 5.2733e-05, 1.3232e-04,  ..., 1.2967e-04,\n",
      "          2.9757e-04, 7.7447e-05],\n",
      "         [4.1854e-05, 6.4058e-05, 1.5042e-04,  ..., 1.7971e-04,\n",
      "          2.2435e-04, 5.4969e-05]],\n",
      "\n",
      "        [[4.5492e-05, 8.6252e-05, 8.1883e-05,  ..., 1.8220e-04,\n",
      "          9.0744e-05, 4.7950e-05],\n",
      "         [7.2902e-05, 6.6233e-05, 5.0020e-05,  ..., 6.1686e-05,\n",
      "          6.8586e-05, 1.3301e-04],\n",
      "         [4.7247e-05, 9.6153e-05, 4.0729e-05,  ..., 4.0880e-04,\n",
      "          1.5156e-04, 2.4178e-05],\n",
      "         ...,\n",
      "         [6.1130e-05, 1.1843e-04, 4.9155e-05,  ..., 9.2315e-05,\n",
      "          7.8301e-05, 9.7284e-05],\n",
      "         [7.0290e-05, 6.1559e-05, 6.9254e-05,  ..., 1.1955e-04,\n",
      "          1.0324e-04, 3.3989e-05],\n",
      "         [1.0797e-04, 1.3919e-04, 4.2588e-05,  ..., 1.4406e-04,\n",
      "          1.4756e-04, 9.1400e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[5.9059e-05, 1.0147e-04, 1.2578e-04,  ..., 1.9887e-04,\n",
      "          8.0472e-05, 6.4652e-05],\n",
      "         [2.2868e-05, 2.6717e-04, 4.4245e-05,  ..., 1.7694e-04,\n",
      "          8.3841e-05, 1.5313e-05],\n",
      "         [6.4673e-05, 1.3820e-04, 4.7894e-05,  ..., 7.8513e-05,\n",
      "          9.4638e-05, 5.0224e-05],\n",
      "         ...,\n",
      "         [4.1565e-05, 7.4174e-05, 1.2010e-04,  ..., 2.1488e-04,\n",
      "          1.5373e-04, 8.0502e-05],\n",
      "         [2.1435e-05, 7.6648e-05, 8.3861e-05,  ..., 2.3912e-04,\n",
      "          1.0276e-04, 4.3391e-05],\n",
      "         [5.3715e-05, 7.0557e-05, 8.5723e-05,  ..., 1.9938e-04,\n",
      "          8.1816e-05, 5.2467e-05]],\n",
      "\n",
      "        [[3.6031e-05, 7.5166e-05, 8.7478e-05,  ..., 2.2321e-04,\n",
      "          7.7608e-05, 5.3763e-05],\n",
      "         [3.0235e-05, 1.0212e-04, 9.1669e-05,  ..., 1.2031e-04,\n",
      "          4.6535e-05, 4.3793e-05],\n",
      "         [3.9889e-05, 3.5505e-05, 5.8466e-05,  ..., 1.6781e-04,\n",
      "          2.2657e-04, 5.1734e-05],\n",
      "         ...,\n",
      "         [3.2570e-05, 4.4765e-05, 1.1586e-04,  ..., 9.1481e-05,\n",
      "          9.9528e-05, 5.5813e-05],\n",
      "         [1.8357e-05, 1.0257e-04, 9.5448e-05,  ..., 3.9135e-05,\n",
      "          1.0682e-04, 5.3014e-05],\n",
      "         [6.7343e-05, 7.0358e-05, 2.1406e-04,  ..., 2.9097e-04,\n",
      "          1.7447e-04, 9.1409e-05]],\n",
      "\n",
      "        [[3.8929e-05, 9.7364e-05, 1.0075e-04,  ..., 2.0627e-04,\n",
      "          8.7524e-05, 9.2944e-05],\n",
      "         [9.1620e-05, 6.9777e-05, 3.5692e-05,  ..., 2.6017e-04,\n",
      "          1.7612e-04, 3.4235e-05],\n",
      "         [4.6481e-05, 9.8704e-05, 7.8933e-05,  ..., 2.3813e-04,\n",
      "          7.1748e-05, 2.9013e-05],\n",
      "         ...,\n",
      "         [2.3587e-05, 9.9598e-05, 5.5273e-05,  ..., 2.4724e-04,\n",
      "          2.9308e-04, 2.5494e-05],\n",
      "         [1.0927e-04, 9.2366e-05, 5.9606e-05,  ..., 2.0667e-04,\n",
      "          2.0162e-04, 7.2793e-05],\n",
      "         [6.8287e-05, 9.4102e-05, 4.9553e-05,  ..., 2.6152e-04,\n",
      "          1.8313e-04, 3.2973e-05]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor(9.2104, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T12:56:43.918789Z",
     "start_time": "2024-11-18T12:56:43.825499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self, model, source_tokenizer, target_tokenizer, search_strategy=\"greedy\"):\n",
    "        self.model = model\n",
    "        self.source_tokenizer = source_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "        \n",
    "        if search_strategy == \"greedy\":\n",
    "            self.search_strategy = self.greedy_search\n",
    "        \n",
    "    def translate(self, text, max_length=None):\n",
    "        source_tokens = self.source_tokenizer(text, truncation=True, padding=\"max_length\", max_length=64).input_ids\n",
    "        source_tokens = torch.tensor(source_tokens).unsqueeze(0).to(next(self.model.parameters()).device)\n",
    "        \n",
    "        if max_length is None:\n",
    "            max_length = self.model.config[\"max_seq_len\"]\n",
    "        \n",
    "        encoder_output = self.model.encoder(source_tokens)\n",
    "        \n",
    "        translated_tokens = torch.empty(source_tokens.size()[0], 0, dtype=torch.long).to(source_tokens.device)\n",
    "        for i in range(max_length):\n",
    "            predictions = self.model.decoder(translated_tokens, encoder_output)\n",
    "            translated_tokens = self.search_strategy(translated_tokens, predictions)\n",
    "            if torch.all(translated_tokens[:, -1] == self.target_tokenizer.eos_token_id):\n",
    "                break\n",
    "        \n",
    "        return self.target_tokenizer.batch_decode(translated_tokens)\n",
    "        \n",
    "    def greedy_search(self, translated_tokens, predictions):\n",
    "        return torch.cat([translated_tokens, torch.argmax(predictions[:, -1, :], dim=-1).unsqueeze(0)], dim=-1)\n",
    "\n",
    "\n",
    "source_tokenizer = AutoTokenizer.from_pretrained(\"preprocessed_dataset/source_tokenizer\")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"preprocessed_dataset/target_tokenizer\")\n",
    "source_tokenizer.pad_token = \"<pad>\" \n",
    "translator = Translator(transformer, source_tokenizer, target_tokenizer) \n",
    "\n",
    "translator.translate(\"hello\")"
   ],
   "id": "4206c450a9e8f031",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:49:50.756153Z",
     "start_time": "2024-11-12T18:49:50.703780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(transformer.decoder.embedding.token_embedding.weight.data[10])\n",
    "print(transformer.decoder.embedding.token_embedding.weight.data[20])\n",
    "\n",
    "print(transformer.decoder.embedding.positional_embedding[0, 0])\n",
    "print(transformer.decoder.embedding.positional_embedding[0, 2])\n"
   ],
   "id": "7cd7cb77955aea32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9029, -0.2277,  0.8121,  0.4913, -0.1294,  2.2102,  1.9592,  0.1228,\n",
      "        -0.1017, -0.0548,  0.7717, -0.7806, -1.6557,  0.2264, -0.1933,  0.7390,\n",
      "         0.2000, -0.1708, -2.0923,  0.1391, -0.3606,  0.5193,  1.0636,  0.1898,\n",
      "        -0.5848, -1.8388, -1.0696,  1.1762, -1.6640, -1.3206,  0.3680, -1.3823,\n",
      "         1.7483,  1.2942, -0.7588, -0.7291, -1.2261, -0.8970,  0.8327,  1.1333,\n",
      "         1.5878,  0.0891,  0.5280,  0.3989,  0.1955, -1.0455,  0.4364,  1.1388,\n",
      "        -0.7819,  1.3072, -0.8736, -0.3792, -0.8247, -1.4188, -1.0088, -0.7348,\n",
      "         1.9078, -1.1012, -0.7090,  0.3916, -0.3001,  0.3209,  0.2635,  0.8543,\n",
      "         0.5985,  0.0758, -0.6523,  0.4882, -0.0912,  0.5008,  1.1756, -0.2584,\n",
      "         0.3211,  0.3765, -1.2757,  0.8564,  0.0277, -0.1435,  0.3744, -1.2294,\n",
      "        -0.7166, -0.2129, -0.9265,  0.0932, -1.9291,  1.2988,  0.0140, -0.0414,\n",
      "         1.9615, -0.4395, -0.6023,  0.3582, -0.6587,  0.5204,  0.0948,  0.9241,\n",
      "        -0.4053, -0.6732, -1.0680, -0.2942,  1.2884,  0.3919,  0.9617,  0.7011,\n",
      "        -1.3738,  0.8780,  0.1179, -0.2492,  0.4989, -0.7233,  0.5135,  0.5251,\n",
      "        -1.9990, -1.1495, -2.0208,  0.0483,  1.0029, -0.6316, -2.1933,  0.2776,\n",
      "        -0.9443, -0.7690,  0.6458, -0.2765, -1.3312, -1.2529,  0.4793,  0.1897,\n",
      "         0.2274, -1.2922,  1.2461, -0.7350, -1.4161, -1.4955,  1.0532,  0.4879,\n",
      "        -0.1599, -0.0889, -0.6589, -1.4590,  1.6704, -1.7407,  0.6839, -0.8462,\n",
      "        -1.1426, -1.5916, -0.2424, -1.1179, -0.9235,  1.2713, -0.6873,  0.2910,\n",
      "         1.3577, -0.0484,  1.2980, -0.7188,  0.0139, -0.1168, -0.8350, -2.3072,\n",
      "        -0.4997, -1.6193, -0.0355,  1.2868, -1.5247,  0.2805,  0.4187,  1.9285,\n",
      "        -1.1534,  0.5022, -1.7632,  0.1581, -0.3518, -0.0431,  1.8766, -0.2290,\n",
      "        -1.5433, -1.2701, -0.8496,  1.5204,  0.8005, -0.2380, -1.0900, -1.3227,\n",
      "        -0.0408, -0.7750,  0.3676, -0.4150,  0.8529,  1.0581,  2.0055, -1.7755,\n",
      "        -0.4670, -1.0874,  0.5640,  0.6446, -0.3358, -2.1674, -0.2262, -0.9088,\n",
      "         0.5101,  1.2345, -0.0060, -0.6682, -0.6208, -0.8224,  0.6873,  0.6731,\n",
      "         0.0072,  1.3177,  0.5123, -0.4937,  0.8211,  0.7684, -0.8376,  0.5814,\n",
      "        -0.3165,  1.5401,  1.0701,  1.7621, -0.5202,  1.6809,  1.0193, -2.2374,\n",
      "        -0.5025, -0.7868,  1.1477,  0.5912, -0.6267, -0.4664,  0.8775,  0.5436,\n",
      "        -0.6709,  0.4943, -0.3206,  1.0912, -0.7938, -1.4669, -1.1674,  1.9206,\n",
      "        -1.4930,  0.7901, -0.2976,  1.6661,  0.9250, -0.4886, -0.1987,  0.4492,\n",
      "         1.0422,  0.5866,  0.1687, -1.1104, -0.5981,  0.4093,  0.3794,  0.5208],\n",
      "       device='cuda:0')\n",
      "tensor([-0.8968, -0.0439,  0.6451, -0.6762,  0.2378,  0.3137, -1.1435, -0.3093,\n",
      "         0.7133, -0.3173, -0.0074, -0.7529, -0.9300, -0.9326, -1.1514, -1.2086,\n",
      "         1.5134,  1.2339,  0.1490,  0.2353, -1.2278, -0.1442,  2.2986, -0.8930,\n",
      "        -0.1056, -1.3822, -0.9146,  1.3916, -0.2490,  0.4223,  0.0094,  1.4541,\n",
      "         0.9151,  0.8383, -0.5847, -0.4340,  1.2025,  0.1571, -0.0525, -0.6861,\n",
      "         0.2714,  0.3490, -2.0081, -0.2198,  1.7818,  2.1224, -0.2631,  0.7077,\n",
      "        -0.2585,  0.2605,  0.7111, -0.9202, -2.6339,  0.6734, -1.1534, -0.5952,\n",
      "         0.7190,  0.9055,  0.4521,  0.4083,  0.3186, -0.6491,  0.6803,  2.4117,\n",
      "        -0.0285,  0.5609, -1.3510,  0.4371, -0.3815, -0.2105, -1.3168,  0.4233,\n",
      "        -0.3852, -0.7635,  0.5926, -0.0493,  0.1056, -0.1568, -0.6373,  0.4620,\n",
      "         0.6774, -0.3616,  0.1210, -0.1194, -0.2139,  0.8564,  1.6952,  0.4705,\n",
      "        -0.4293, -1.4901,  1.0656, -1.6738, -0.1354, -0.3580, -0.1413,  0.3344,\n",
      "        -0.6055,  1.4425, -0.3537, -0.0503, -0.6945, -0.5627, -0.7003, -0.7770,\n",
      "         0.9868,  0.0706, -0.4431,  2.3529, -1.4871, -0.6246,  1.3990, -0.7116,\n",
      "        -0.9064,  0.2876, -0.1236,  0.7947, -0.0585,  1.6585, -0.1684,  1.0921,\n",
      "         1.0461, -0.1934,  0.1143,  0.9033,  0.4904,  0.3670,  1.2355,  0.2945,\n",
      "        -0.0738,  0.2125, -0.6118, -1.1143,  0.7623,  0.2630, -1.4076,  0.8366,\n",
      "         1.7163, -0.0405,  2.0657, -0.2316, -1.9616,  1.6481,  0.1049, -0.1003,\n",
      "         0.2968, -1.0028, -0.9184, -0.5756, -0.6150,  0.8330,  0.1898,  0.6162,\n",
      "        -0.8493, -0.5266, -3.1597, -1.0467,  0.5498,  0.8158,  0.0891, -2.3173,\n",
      "         0.4057,  0.2606, -1.6118, -0.3394, -0.8546, -0.7822,  0.0516, -2.9214,\n",
      "         2.4398, -0.8993, -0.2283, -0.4793, -1.8538, -0.1719, -0.7418, -1.2661,\n",
      "        -0.2709, -1.1706, -0.0687, -0.5646,  0.0317, -0.0041, -1.8051,  0.4788,\n",
      "        -1.1365,  0.3150, -0.9878, -0.4175,  1.6608, -1.6904,  1.4331,  0.8742,\n",
      "         0.8805, -0.1700, -0.0394, -1.3136,  0.4119,  0.3541, -0.7618, -0.7399,\n",
      "         0.1332, -1.3624, -1.1515, -2.1679, -0.0919, -0.7606, -0.7849,  1.2363,\n",
      "        -0.4362, -0.4570,  0.3175,  0.8012,  1.1810, -0.5817,  0.7135, -0.2748,\n",
      "        -1.0379, -0.0417, -2.1565, -1.3434, -1.9421, -0.8345, -1.3399,  0.2384,\n",
      "        -0.8026, -0.7787,  0.6018,  0.2912, -1.2105, -0.7793,  0.5033, -0.3921,\n",
      "         0.4238,  0.4203, -0.5320,  1.6044, -0.0137, -0.9956,  0.3122, -0.0215,\n",
      "        -0.3521,  0.2332, -0.9150,  0.2377, -0.8725, -0.7029, -0.3345, -1.6177,\n",
      "        -0.1457,  0.7764, -1.1835, -0.5812,  1.1109, -0.2721, -0.7797,  0.4009],\n",
      "       device='cuda:0')\n",
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1.], device='cuda:0')\n",
      "tensor([ 9.0930e-01, -4.1615e-01,  9.5814e-01, -2.8629e-01,  9.8705e-01,\n",
      "        -1.6044e-01,  9.9916e-01, -4.0877e-02,  9.9748e-01,  7.0948e-02,\n",
      "         9.8470e-01,  1.7424e-01,  9.6323e-01,  2.6869e-01,  9.3512e-01,\n",
      "         3.5434e-01,  9.0213e-01,  4.3146e-01,  8.6573e-01,  5.0052e-01,\n",
      "         8.2710e-01,  5.6205e-01,  7.8724e-01,  6.1665e-01,  7.4690e-01,\n",
      "         6.6493e-01,  7.0671e-01,  7.0750e-01,  6.6713e-01,  7.4494e-01,\n",
      "         6.2851e-01,  7.7780e-01,  5.9113e-01,  8.0658e-01,  5.5515e-01,\n",
      "         8.3175e-01,  5.2071e-01,  8.5373e-01,  4.8788e-01,  8.7291e-01,\n",
      "         4.5669e-01,  8.8962e-01,  4.2716e-01,  9.0418e-01,  3.9926e-01,\n",
      "         9.1684e-01,  3.7295e-01,  9.2785e-01,  3.4821e-01,  9.3742e-01,\n",
      "         3.2495e-01,  9.4573e-01,  3.0314e-01,  9.5295e-01,  2.8269e-01,\n",
      "         9.5921e-01,  2.6355e-01,  9.6464e-01,  2.4565e-01,  9.6936e-01,\n",
      "         2.2891e-01,  9.7345e-01,  2.1327e-01,  9.7699e-01,  1.9867e-01,\n",
      "         9.8007e-01,  1.8504e-01,  9.8273e-01,  1.7233e-01,  9.8504e-01,\n",
      "         1.6047e-01,  9.8704e-01,  1.4942e-01,  9.8877e-01,  1.3911e-01,\n",
      "         9.9028e-01,  1.2951e-01,  9.9158e-01,  1.2057e-01,  9.9271e-01,\n",
      "         1.1223e-01,  9.9368e-01,  1.0447e-01,  9.9453e-01,  9.7240e-02,\n",
      "         9.9526e-01,  9.0508e-02,  9.9590e-01,  8.4239e-02,  9.9645e-01,\n",
      "         7.8403e-02,  9.9692e-01,  7.2970e-02,  9.9733e-01,  6.7912e-02,\n",
      "         9.9769e-01,  6.3203e-02,  9.9800e-01,  5.8821e-02,  9.9827e-01,\n",
      "         5.4741e-02,  9.9850e-01,  5.0944e-02,  9.9870e-01,  4.7410e-02,\n",
      "         9.9888e-01,  4.4120e-02,  9.9903e-01,  4.1059e-02,  9.9916e-01,\n",
      "         3.8210e-02,  9.9927e-01,  3.5558e-02,  9.9937e-01,  3.3090e-02,\n",
      "         9.9945e-01,  3.0794e-02,  9.9953e-01,  2.8656e-02,  9.9959e-01,\n",
      "         2.6667e-02,  9.9964e-01,  2.4816e-02,  9.9969e-01,  2.3094e-02,\n",
      "         9.9973e-01,  2.1490e-02,  9.9977e-01,  1.9999e-02,  9.9980e-01,\n",
      "         1.8610e-02,  9.9983e-01,  1.7318e-02,  9.9985e-01,  1.6116e-02,\n",
      "         9.9987e-01,  1.4997e-02,  9.9989e-01,  1.3956e-02,  9.9990e-01,\n",
      "         1.2987e-02,  9.9992e-01,  1.2086e-02,  9.9993e-01,  1.1247e-02,\n",
      "         9.9994e-01,  1.0466e-02,  9.9995e-01,  9.7392e-03,  9.9995e-01,\n",
      "         9.0630e-03,  9.9996e-01,  8.4338e-03,  9.9996e-01,  7.8483e-03,\n",
      "         9.9997e-01,  7.3034e-03,  9.9997e-01,  6.7964e-03,  9.9998e-01,\n",
      "         6.3245e-03,  9.9998e-01,  5.8854e-03,  9.9998e-01,  5.4768e-03,\n",
      "         9.9998e-01,  5.0966e-03,  9.9999e-01,  4.7427e-03,  9.9999e-01,\n",
      "         4.4135e-03,  9.9999e-01,  4.1070e-03,  9.9999e-01,  3.8219e-03,\n",
      "         9.9999e-01,  3.5566e-03,  9.9999e-01,  3.3096e-03,  9.9999e-01,\n",
      "         3.0798e-03,  1.0000e+00,  2.8660e-03,  1.0000e+00,  2.6670e-03,\n",
      "         1.0000e+00,  2.4819e-03,  1.0000e+00,  2.3096e-03,  1.0000e+00,\n",
      "         2.1492e-03,  1.0000e+00,  2.0000e-03,  1.0000e+00,  1.8611e-03,\n",
      "         1.0000e+00,  1.7319e-03,  1.0000e+00,  1.6117e-03,  1.0000e+00,\n",
      "         1.4998e-03,  1.0000e+00,  1.3957e-03,  1.0000e+00,  1.2988e-03,\n",
      "         1.0000e+00,  1.2086e-03,  1.0000e+00,  1.1247e-03,  1.0000e+00,\n",
      "         1.0466e-03,  1.0000e+00,  9.7393e-04,  1.0000e+00,  9.0632e-04,\n",
      "         1.0000e+00,  8.4339e-04,  1.0000e+00,  7.8484e-04,  1.0000e+00,\n",
      "         7.3035e-04,  1.0000e+00,  6.7964e-04,  1.0000e+00,  6.3246e-04,\n",
      "         1.0000e+00,  5.8855e-04,  1.0000e+00,  5.4768e-04,  1.0000e+00,\n",
      "         5.0966e-04,  1.0000e+00,  4.7427e-04,  1.0000e+00,  4.4135e-04,\n",
      "         1.0000e+00,  4.1070e-04,  1.0000e+00,  3.8219e-04,  1.0000e+00,\n",
      "         3.5566e-04,  1.0000e+00,  3.3096e-04,  1.0000e+00,  3.0799e-04,\n",
      "         1.0000e+00,  2.8660e-04,  1.0000e+00,  2.6670e-04,  1.0000e+00,\n",
      "         2.4819e-04,  1.0000e+00,  2.3096e-04,  1.0000e+00,  2.1492e-04,\n",
      "         1.0000e+00], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c09e2132b7e9c7c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
